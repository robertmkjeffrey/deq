|Basic Memory Usage: 8.69MB current, 8.69MB max
====================================================================================================
    - data : ../data/penn/
    - dataset : ptb
    - n_layer : 20
    - d_embed : 400
    - nhid : 1000
    - nout : 400
    - epochs : 500
    - halt_ppl : None
    - time_limit : 125000.0
    - optim : Adam
    - lr : 0.01
    - clip : 0.225
    - batch_size : 32
    - batch_chunk : 1
    - seq_len : 110
    - subseq_len : 55
    - dropout : 0.45
    - dropouti : 0.45
    - wdrop : 0.5
    - emb_dropout : 0.1
    - dropouth : 0.28
    - weight_decay : 1.2e-06
    - wnorm : True
    - seed : 9999
    - cuda : True
    - not_tied : False
    - anneal : 10
    - log_interval : 100
    - force_deq_validation : False
    - when : [-1]
    - ksize : 2
    - dilation : 1
    - n_experts : 0
    - multi_gpu : False
    - use_gpus : None
    - f_thres : 45
    - b_thres : 45
    - work_dir : LM-TREdeq-ptb/mem_20l_20210529-223429
    - restart : False
    - restart_dir : 
    - debug : False
    - gpu0_bsz : 7
    - pretrain_steps : 15000
    - start_train_steps : 0
    - timing : True
    - eval_mem : True
    - eval : False
    - load : 
    - name : mem_20l
    - tied : True
    - n_token : 10000
    - n_all_param : 24193603
====================================================================================================
#params = 24193603
|Static Memory Usage: 252.25MB current, 269.03MB max
| epoch  -1 step      100 |    100 batches | lr 0.01 | ms/batch 560.15 | loss  6.45 | ppl   635.783
| epoch  -1 step      200 |    200 batches | lr 0.01 | ms/batch 558.04 | loss  5.83 | ppl   341.236
| train loss: 6.04 | train ppl 418.452
|Training Memory Usage: 3783.48MB
|Training Inference Memory Usage: 1053.80MB
|Validation Memory Usage: 703.95MB
|Test Memory Usage: 703.95MB
