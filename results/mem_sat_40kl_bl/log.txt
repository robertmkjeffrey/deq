|Basic Memory Usage: 8.69MB current, 8.69MB max
====================================================================================================
    - data : ../data/penn/
    - dataset : ptb
    - n_layer : 40
    - d_embed : 400
    - nhid : 1000
    - nout : 400
    - epochs : 500
    - halt_ppl : None
    - time_limit : 16000.0
    - optim : Adam
    - lr : 0.01
    - clip : 0.225
    - batch_size : 32
    - batch_chunk : 1
    - seq_len : 110
    - subseq_len : 55
    - dropout : 0.45
    - dropouti : 0.45
    - wdrop : 0.5
    - emb_dropout : 0.1
    - dropouth : 0.28
    - weight_decay : 1.2e-06
    - wnorm : True
    - seed : 500
    - cuda : True
    - not_tied : False
    - anneal : 10
    - log_interval : 100
    - force_deq_validation : False
    - when : [-1]
    - ksize : 2
    - dilation : 1
    - n_experts : 0
    - multi_gpu : False
    - use_gpus : None
    - f_thres : 45
    - b_thres : 45
    - work_dir : LM-TREdeq-ptb/mem_sat_40kl_20210602-165259
    - restart : False
    - restart_dir : 
    - debug : False
    - gpu0_bsz : -1
    - pretrain_steps : 5000
    - start_train_steps : 0
    - timing : True
    - eval_mem : False
    - eval : False
    - load : 
    - name : mem_sat_40kl
    - tied : True
    - n_token : 10000
    - n_all_param : 24193603
====================================================================================================
#params = 24193603
|Static Memory Usage: 252.25MB current, 269.03MB max
| epoch   1 step      100 |    100 batches | lr 0.01 | ms/batch 1105.73 | loss  6.40 | ppl   599.680
| epoch   1 step      200 |    200 batches | lr 0.01 | ms/batch 1096.04 | loss  5.82 | ppl   337.284
| train loss: 6.01 | train ppl 407.222
----------------------------------------------------------------------------------------------------
| Eval   1 at step      265 | time: 300.87s | valid loss  5.44 | valid ppl   230.475
| Total time: 300.55s
----------------------------------------------------------------------------------------------------
| epoch   2 step      300 |     35 batches | lr 0.01 | ms/batch 1205.05 | loss  1.98 | ppl     7.235
| epoch   2 step      400 |    135 batches | lr 0.01 | ms/batch 1095.37 | loss  5.54 | ppl   254.418
| epoch   2 step      500 |    235 batches | lr 0.01 | ms/batch 1094.83 | loss  5.41 | ppl   224.308
| train loss: 5.50 | train ppl 243.694
----------------------------------------------------------------------------------------------------
| Eval   2 at step      530 | time: 301.16s | valid loss  5.17 | valid ppl   175.047
| Total time: 601.96s
----------------------------------------------------------------------------------------------------
| epoch   3 step      600 |     70 batches | lr 0.01 | ms/batch 1205.99 | loss  3.76 | ppl    42.789
| epoch   3 step      700 |    170 batches | lr 0.01 | ms/batch 1096.10 | loss  5.31 | ppl   202.736
| train loss: 5.30 | train ppl 200.762
----------------------------------------------------------------------------------------------------
| Eval   3 at step      795 | time: 301.45s | valid loss  5.01 | valid ppl   150.467
| Total time: 903.64s
----------------------------------------------------------------------------------------------------
| epoch   4 step      800 |      5 batches | lr 0.01 | ms/batch 1206.21 | loss  0.27 | ppl     1.314
| epoch   4 step      900 |    105 batches | lr 0.01 | ms/batch 1096.23 | loss  5.23 | ppl   186.331
| epoch   4 step     1000 |    205 batches | lr 0.01 | ms/batch 1096.08 | loss  5.14 | ppl   171.101
| train loss: 5.19 | train ppl 178.938
----------------------------------------------------------------------------------------------------
| Eval   4 at step     1060 | time: 301.51s | valid loss  4.93 | valid ppl   137.796
| Total time: 1205.41s
----------------------------------------------------------------------------------------------------
| epoch   5 step     1100 |     40 batches | lr 0.01 | ms/batch 1206.63 | loss  2.06 | ppl     7.845
| epoch   5 step     1200 |    140 batches | lr 0.01 | ms/batch 1096.11 | loss  5.10 | ppl   164.417
| epoch   5 step     1300 |    240 batches | lr 0.01 | ms/batch 1096.23 | loss  5.04 | ppl   154.575
| train loss: 5.09 | train ppl 161.852
----------------------------------------------------------------------------------------------------
| Eval   5 at step     1325 | time: 301.52s | valid loss  4.86 | valid ppl   128.445
| Total time: 1507.14s
----------------------------------------------------------------------------------------------------
| epoch   6 step     1400 |     75 batches | lr 0.01 | ms/batch 1206.32 | loss  3.79 | ppl    44.460
| epoch   6 step     1500 |    175 batches | lr 0.01 | ms/batch 1095.91 | loss  5.01 | ppl   149.451
| train loss: 5.01 | train ppl 150.366
----------------------------------------------------------------------------------------------------
| Eval   6 at step     1590 | time: 301.46s | valid loss  4.81 | valid ppl   123.217
| Total time: 1808.85s
----------------------------------------------------------------------------------------------------
| epoch   7 step     1600 |     10 batches | lr 0.01 | ms/batch 1206.43 | loss  0.51 | ppl     1.669
| epoch   7 step     1700 |    110 batches | lr 0.01 | ms/batch 1095.84 | loss  4.98 | ppl   145.802
| epoch   7 step     1800 |    210 batches | lr 0.01 | ms/batch 1096.00 | loss  4.92 | ppl   137.442
| train loss: 4.96 | train ppl 142.493
----------------------------------------------------------------------------------------------------
| Eval   7 at step     1855 | time: 301.46s | valid loss  4.78 | valid ppl   119.146
| Total time: 2110.52s
----------------------------------------------------------------------------------------------------
| epoch   8 step     1900 |     45 batches | lr 0.01 | ms/batch 1206.19 | loss  2.23 | ppl     9.258
| epoch   8 step     2000 |    145 batches | lr 0.01 | ms/batch 1096.04 | loss  4.93 | ppl   138.291
| epoch   8 step     2100 |    245 batches | lr 0.01 | ms/batch 1095.92 | loss  4.86 | ppl   129.032
| train loss: 4.91 | train ppl 135.473
----------------------------------------------------------------------------------------------------
| Eval   8 at step     2120 | time: 301.48s | valid loss  4.75 | valid ppl   115.932
| Total time: 2412.15s
----------------------------------------------------------------------------------------------------
| epoch   9 step     2200 |     80 batches | lr 0.01 | ms/batch 1206.35 | loss  3.92 | ppl    50.218
| epoch   9 step     2300 |    180 batches | lr 0.01 | ms/batch 1095.74 | loss  4.85 | ppl   128.294
| train loss: 4.86 | train ppl 129.240
----------------------------------------------------------------------------------------------------
| Eval   9 at step     2385 | time: 301.44s | valid loss  4.73 | valid ppl   112.878
| Total time: 2713.83s
----------------------------------------------------------------------------------------------------
| epoch  10 step     2400 |     15 batches | lr 0.01 | ms/batch 1206.31 | loss  0.74 | ppl     2.095
| epoch  10 step     2500 |    115 batches | lr 0.01 | ms/batch 1095.87 | loss  4.85 | ppl   127.206
| epoch  10 step     2600 |    215 batches | lr 0.01 | ms/batch 1096.06 | loss  4.78 | ppl   118.543
| train loss: 4.82 | train ppl 123.990
----------------------------------------------------------------------------------------------------
| Eval  10 at step     2650 | time: 301.47s | valid loss  4.72 | valid ppl   112.687
| Total time: 3015.52s
----------------------------------------------------------------------------------------------------
| epoch  11 step     2700 |     50 batches | lr 0.01 | ms/batch 1206.55 | loss  2.41 | ppl    11.101
| epoch  11 step     2800 |    150 batches | lr 0.01 | ms/batch 1095.93 | loss  4.81 | ppl   122.384
| epoch  11 step     2900 |    250 batches | lr 0.01 | ms/batch 1096.09 | loss  4.74 | ppl   114.960
| train loss: 4.79 | train ppl 119.884
----------------------------------------------------------------------------------------------------
| Eval  11 at step     2915 | time: 301.50s | valid loss  4.68 | valid ppl   108.224
| Total time: 3317.27s
----------------------------------------------------------------------------------------------------
| epoch  12 step     3000 |     85 batches | lr 0.01 | ms/batch 1206.18 | loss  4.07 | ppl    58.583
| epoch  12 step     3100 |    185 batches | lr 0.01 | ms/batch 1095.93 | loss  4.75 | ppl   115.940
| train loss: 4.77 | train ppl 117.358
----------------------------------------------------------------------------------------------------
| Eval  12 at step     3180 | time: 301.42s | valid loss  4.66 | valid ppl   106.106
| Total time: 3618.94s
----------------------------------------------------------------------------------------------------
| epoch  13 step     3200 |     20 batches | lr 0.01 | ms/batch 1205.95 | loss  0.96 | ppl     2.610
| epoch  13 step     3300 |    120 batches | lr 0.01 | ms/batch 1095.97 | loss  4.76 | ppl   117.035
| epoch  13 step     3400 |    220 batches | lr 0.01 | ms/batch 1095.84 | loss  4.68 | ppl   107.532
| train loss: 4.73 | train ppl 113.626
----------------------------------------------------------------------------------------------------
| Eval  13 at step     3445 | time: 301.45s | valid loss  4.65 | valid ppl   104.220
| Total time: 3920.57s
----------------------------------------------------------------------------------------------------
| epoch  14 step     3500 |     55 batches | lr 0.01 | ms/batch 1206.35 | loss  2.61 | ppl    13.548
| epoch  14 step     3600 |    155 batches | lr 0.01 | ms/batch 1095.93 | loss  4.73 | ppl   113.819
| epoch  14 step     3700 |    255 batches | lr 0.01 | ms/batch 1096.16 | loss  4.69 | ppl   108.495
| train loss: 4.72 | train ppl 112.071
----------------------------------------------------------------------------------------------------
| Eval  14 at step     3710 | time: 301.56s | valid loss  4.64 | valid ppl   103.944
| Total time: 4222.21s
----------------------------------------------------------------------------------------------------
| epoch  15 step     3800 |     90 batches | lr 0.01 | ms/batch 1207.64 | loss  4.25 | ppl    70.004
| epoch  15 step     3900 |    190 batches | lr 0.01 | ms/batch 1095.99 | loss  4.66 | ppl   105.753
| train loss: 4.69 | train ppl 109.145
----------------------------------------------------------------------------------------------------
| Eval  15 at step     3975 | time: 301.53s | valid loss  4.61 | valid ppl   100.257
| Total time: 4523.97s
----------------------------------------------------------------------------------------------------
| epoch  16 step     4000 |     25 batches | lr 0.01 | ms/batch 1206.52 | loss  1.18 | ppl     3.243
| epoch  16 step     4100 |    125 batches | lr 0.01 | ms/batch 1096.07 | loss  4.69 | ppl   108.953
| epoch  16 step     4200 |    225 batches | lr 0.01 | ms/batch 1096.07 | loss  4.62 | ppl   101.465
| train loss: 4.67 | train ppl 106.319
----------------------------------------------------------------------------------------------------
| Eval  16 at step     4240 | time: 301.50s | valid loss  4.61 | valid ppl   100.320
| Total time: 4825.52s
----------------------------------------------------------------------------------------------------
| epoch  17 step     4300 |     60 batches | lr 0.01 | ms/batch 1188.64 | loss  2.80 | ppl    16.490
| epoch  17 step     4400 |    160 batches | lr 0.01 | ms/batch 1093.01 | loss  4.67 | ppl   107.000
| epoch  17 step     4500 |    260 batches | lr 0.01 | ms/batch 1093.62 | loss  4.63 | ppl   102.278
| train loss: 4.66 | train ppl 105.242
----------------------------------------------------------------------------------------------------
| Eval  17 at step     4505 | time: 299.16s | valid loss  4.61 | valid ppl   100.319
| Total time: 5124.84s
----------------------------------------------------------------------------------------------------
| epoch  18 step     4600 |     95 batches | lr 0.01 | ms/batch 1187.92 | loss  4.43 | ppl    84.201
| epoch  18 step     4700 |    195 batches | lr 0.01 | ms/batch 1092.78 | loss  4.61 | ppl   100.112
| train loss: 4.64 | train ppl 103.520
----------------------------------------------------------------------------------------------------
| Eval  18 at step     4770 | time: 299.12s | valid loss  4.62 | valid ppl   101.551
| Total time: 5424.30s
----------------------------------------------------------------------------------------------------
| epoch  19 step     4800 |     30 batches | lr 0.01 | ms/batch 1187.67 | loss  1.40 | ppl     4.051
| epoch  19 step     4900 |    130 batches | lr 0.01 | ms/batch 1092.94 | loss  4.65 | ppl   104.447
| epoch  19 step     5000 |    230 batches | lr 0.01 | ms/batch 1094.16 | loss  4.59 | ppl    98.343
| train loss: 4.63 | train ppl 102.709
!! Converted Model !!
----------------------------------------------------------------------------------------------------
| Eval  19 at step     5035 | time: 432.60s | valid loss  4.59 | valid ppl    98.615
| Total time: 5857.29s
----------------------------------------------------------------------------------------------------
| epoch  20 step     5100 |     65 batches | lr 0.01 | ms/batch 3798.49 | loss  3.01 | ppl    20.305
| epoch  20 step     5200 |    165 batches | lr 0.01 | ms/batch 3030.83 | loss  4.64 | ppl   103.988
| epoch  20 step     5300 |    265 batches | lr 0.01 | ms/batch 3001.97 | loss  4.65 | ppl   104.390
| train loss: 4.65 | train ppl 104.284
----------------------------------------------------------------------------------------------------
| Eval  20 at step     5300 | time: 879.33s | valid loss  4.70 | valid ppl   110.414
| Total time: 6737.47s
----------------------------------------------------------------------------------------------------
| epoch  21 step     5400 |    100 batches | lr 0.01 | ms/batch 3798.41 | loss  4.75 | ppl   115.852
| epoch  21 step     5500 |    200 batches | lr 0.01 | ms/batch 3026.20 | loss  4.62 | ppl   101.893
| train loss: 4.69 | train ppl 108.542
----------------------------------------------------------------------------------------------------
| Eval  21 at step     5565 | time: 876.47s | valid loss  4.61 | valid ppl   100.526
| Total time: 7614.92s
----------------------------------------------------------------------------------------------------
| epoch  22 step     5600 |     35 batches | lr 0.01 | ms/batch 3772.01 | loss  1.65 | ppl     5.199
| epoch  22 step     5700 |    135 batches | lr 0.01 | ms/batch 3025.76 | loss  4.67 | ppl   106.683
| epoch  22 step     5800 |    235 batches | lr 0.01 | ms/batch 3026.25 | loss  4.61 | ppl   100.716
| train loss: 4.66 | train ppl 105.420
----------------------------------------------------------------------------------------------------
| Eval  22 at step     5830 | time: 876.42s | valid loss  4.61 | valid ppl   100.098
| Total time: 8492.33s
----------------------------------------------------------------------------------------------------
| epoch  23 step     5900 |     70 batches | lr 0.01 | ms/batch 3771.58 | loss  3.26 | ppl    25.988
| epoch  23 step     6000 |    170 batches | lr 0.01 | ms/batch 3025.92 | loss  4.67 | ppl   107.051
| train loss: 4.65 | train ppl 104.333
----------------------------------------------------------------------------------------------------
| Eval  23 at step     6095 | time: 876.40s | valid loss  4.61 | valid ppl   100.007
| Total time: 9369.69s
----------------------------------------------------------------------------------------------------
| epoch  24 step     6100 |      5 batches | lr 0.01 | ms/batch 3770.76 | loss  0.24 | ppl     1.273
| epoch  24 step     6200 |    105 batches | lr 0.01 | ms/batch 3026.29 | loss  4.66 | ppl   105.809
| epoch  24 step     6300 |    205 batches | lr 0.01 | ms/batch 3026.02 | loss  4.62 | ppl   101.500
| train loss: 4.65 | train ppl 104.622
----------------------------------------------------------------------------------------------------
| Eval  24 at step     6360 | time: 876.58s | valid loss  4.60 | valid ppl    99.190
| Total time: 10247.20s
----------------------------------------------------------------------------------------------------
| epoch  25 step     6400 |     40 batches | lr 0.01 | ms/batch 3772.44 | loss  1.88 | ppl     6.534
| epoch  25 step     6500 |    140 batches | lr 0.01 | ms/batch 3026.33 | loss  4.65 | ppl   104.662
| epoch  25 step     6600 |    240 batches | lr 0.01 | ms/batch 3026.20 | loss  4.61 | ppl   100.145
| train loss: 4.64 | train ppl 104.001
----------------------------------------------------------------------------------------------------
| Eval  25 at step     6625 | time: 876.57s | valid loss  4.59 | valid ppl    98.576
| Total time: 11124.66s
----------------------------------------------------------------------------------------------------
| epoch  26 step     6700 |     75 batches | lr 0.01 | ms/batch 3787.61 | loss  3.49 | ppl    32.632
| epoch  26 step     6800 |    175 batches | lr 0.01 | ms/batch 3025.38 | loss  4.65 | ppl   104.500
| train loss: 4.64 | train ppl 103.052
----------------------------------------------------------------------------------------------------
| Eval  26 at step     6890 | time: 877.91s | valid loss  4.58 | valid ppl    97.583
| Total time: 12003.44s
----------------------------------------------------------------------------------------------------
| epoch  27 step     6900 |     10 batches | lr 0.01 | ms/batch 3787.61 | loss  0.48 | ppl     1.612
| epoch  27 step     7000 |    110 batches | lr 0.01 | ms/batch 3025.87 | loss  4.64 | ppl   103.236
| epoch  27 step     7100 |    210 batches | lr 0.01 | ms/batch 3025.80 | loss  4.59 | ppl    98.281
| train loss: 4.63 | train ppl 102.310
----------------------------------------------------------------------------------------------------
| Eval  27 at step     7155 | time: 878.09s | valid loss  4.58 | valid ppl    97.400
| Total time: 12882.23s
----------------------------------------------------------------------------------------------------
| epoch  28 step     7200 |     45 batches | lr 0.01 | ms/batch 3788.92 | loss  2.10 | ppl     8.136
| epoch  28 step     7300 |    145 batches | lr 0.01 | ms/batch 3026.18 | loss  4.63 | ppl   103.028
| epoch  28 step     7400 |    245 batches | lr 0.01 | ms/batch 3026.38 | loss  4.59 | ppl    98.384
| train loss: 4.62 | train ppl 101.924
----------------------------------------------------------------------------------------------------
| Eval  28 at step     7420 | time: 878.22s | valid loss  4.58 | valid ppl    97.744
| Total time: 13761.31s
----------------------------------------------------------------------------------------------------
| epoch  29 step     7500 |     80 batches | lr 0.01 | ms/batch 3771.95 | loss  3.71 | ppl    40.808
| epoch  29 step     7600 |    180 batches | lr 0.01 | ms/batch 3026.25 | loss  4.62 | ppl   101.530
| train loss: 4.62 | train ppl 101.673
----------------------------------------------------------------------------------------------------
| Eval  29 at step     7685 | time: 876.53s | valid loss  4.57 | valid ppl    96.948
| Total time: 14638.57s
----------------------------------------------------------------------------------------------------
| epoch  30 step     7700 |     15 batches | lr 0.01 | ms/batch 3787.68 | loss  0.71 | ppl     2.026
| epoch  30 step     7800 |    115 batches | lr 0.01 | ms/batch 3025.88 | loss  4.64 | ppl   103.488
| epoch  30 step     7900 |    215 batches | lr 0.01 | ms/batch 3026.10 | loss  4.57 | ppl    96.508
| train loss: 4.61 | train ppl 100.972
----------------------------------------------------------------------------------------------------
| Eval  30 at step     7950 | time: 878.02s | valid loss  4.57 | valid ppl    96.250
| Total time: 15517.50s
----------------------------------------------------------------------------------------------------
| epoch  31 step     8000 |     50 batches | lr 0.01 | ms/batch 3787.61 | loss  2.31 | ppl    10.110
| epoch  31 step     8100 |    150 batches | lr 0.01 | ms/batch 3025.94 | loss  4.62 | ppl   101.650
| epoch  31 step     8200 |    250 batches | lr 0.01 | ms/batch 3026.03 | loss  4.58 | ppl    97.727
| train loss: 4.61 | train ppl 100.578
----------------------------------------------------------------------------------------------------
| Eval  31 at step     8215 | time: 878.06s | valid loss  4.58 | valid ppl    97.633
| Total time: 16396.37s
----------------------------------------------------------------------------------------------------
====================================================================================================
| End of training | test loss  4.52 | test ppl    92.128
| Maximum memory usage: 6574.60MB
====================================================================================================
