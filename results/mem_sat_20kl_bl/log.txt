|Basic Memory Usage: 8.69MB current, 8.69MB max
====================================================================================================
    - data : ../data/penn/
    - dataset : ptb
    - n_layer : 20
    - d_embed : 400
    - nhid : 1000
    - nout : 400
    - epochs : 500
    - halt_ppl : None
    - time_limit : 16000.0
    - optim : Adam
    - lr : 0.01
    - clip : 0.225
    - batch_size : 32
    - batch_chunk : 1
    - seq_len : 110
    - subseq_len : 55
    - dropout : 0.45
    - dropouti : 0.45
    - wdrop : 0.5
    - emb_dropout : 0.1
    - dropouth : 0.28
    - weight_decay : 1.2e-06
    - wnorm : True
    - seed : 500
    - cuda : True
    - not_tied : False
    - anneal : 10
    - log_interval : 100
    - force_deq_validation : False
    - when : [-1]
    - ksize : 2
    - dilation : 1
    - n_experts : 0
    - multi_gpu : False
    - use_gpus : None
    - f_thres : 45
    - b_thres : 45
    - work_dir : LM-TREdeq-ptb/mem_sat_20kl_20210602-165401
    - restart : False
    - restart_dir : 
    - debug : False
    - gpu0_bsz : -1
    - pretrain_steps : 5000
    - start_train_steps : 0
    - timing : True
    - eval_mem : False
    - eval : False
    - load : 
    - name : mem_sat_20kl
    - tied : True
    - n_token : 10000
    - n_all_param : 24193603
====================================================================================================
#params = 24193603
|Static Memory Usage: 252.25MB current, 269.03MB max
| epoch   1 step      100 |    100 batches | lr 0.01 | ms/batch 557.78 | loss  6.39 | ppl   594.591
| epoch   1 step      200 |    200 batches | lr 0.01 | ms/batch 553.05 | loss  5.81 | ppl   334.648
| train loss: 6.00 | train ppl 403.745
----------------------------------------------------------------------------------------------------
| Eval   1 at step      265 | time: 152.04s | valid loss  5.41 | valid ppl   224.749
| Total time: 151.84s
----------------------------------------------------------------------------------------------------
| epoch   2 step      300 |     35 batches | lr 0.01 | ms/batch 616.74 | loss  1.97 | ppl     7.200
| epoch   2 step      400 |    135 batches | lr 0.01 | ms/batch 553.14 | loss  5.53 | ppl   251.060
| epoch   2 step      500 |    235 batches | lr 0.01 | ms/batch 553.08 | loss  5.40 | ppl   222.000
| train loss: 5.48 | train ppl 240.721
----------------------------------------------------------------------------------------------------
| Eval   2 at step      530 | time: 152.94s | valid loss  5.15 | valid ppl   172.303
| Total time: 304.78s
----------------------------------------------------------------------------------------------------
| epoch   3 step      600 |     70 batches | lr 0.01 | ms/batch 617.93 | loss  3.74 | ppl    42.256
| epoch   3 step      700 |    170 batches | lr 0.01 | ms/batch 553.08 | loss  5.30 | ppl   199.523
| train loss: 5.29 | train ppl 197.606
----------------------------------------------------------------------------------------------------
| Eval   3 at step      795 | time: 153.04s | valid loss  5.00 | valid ppl   148.565
| Total time: 457.79s
----------------------------------------------------------------------------------------------------
| epoch   4 step      800 |      5 batches | lr 0.01 | ms/batch 618.00 | loss  0.27 | ppl     1.313
| epoch   4 step      900 |    105 batches | lr 0.01 | ms/batch 553.04 | loss  5.21 | ppl   182.875
| epoch   4 step     1000 |    205 batches | lr 0.01 | ms/batch 553.10 | loss  5.13 | ppl   168.183
| train loss: 5.17 | train ppl 176.043
----------------------------------------------------------------------------------------------------
| Eval   4 at step     1060 | time: 153.06s | valid loss  4.91 | valid ppl   135.893
| Total time: 610.81s
----------------------------------------------------------------------------------------------------
| epoch   5 step     1100 |     40 batches | lr 0.01 | ms/batch 618.29 | loss  2.05 | ppl     7.804
| epoch   5 step     1200 |    140 batches | lr 0.01 | ms/batch 553.10 | loss  5.09 | ppl   162.335
| epoch   5 step     1300 |    240 batches | lr 0.01 | ms/batch 553.12 | loss  5.03 | ppl   152.843
| train loss: 5.08 | train ppl 160.057
----------------------------------------------------------------------------------------------------
| Eval   5 at step     1325 | time: 153.09s | valid loss  4.85 | valid ppl   127.276
| Total time: 763.86s
----------------------------------------------------------------------------------------------------
| epoch   6 step     1400 |     75 batches | lr 0.01 | ms/batch 618.07 | loss  3.78 | ppl    44.000
| epoch   6 step     1500 |    175 batches | lr 0.01 | ms/batch 553.21 | loss  5.00 | ppl   147.809
| train loss: 5.00 | train ppl 148.692
----------------------------------------------------------------------------------------------------
| Eval   6 at step     1590 | time: 153.10s | valid loss  4.81 | valid ppl   122.419
| Total time: 916.92s
----------------------------------------------------------------------------------------------------
| epoch   7 step     1600 |     10 batches | lr 0.01 | ms/batch 618.39 | loss  0.51 | ppl     1.668
| epoch   7 step     1700 |    110 batches | lr 0.01 | ms/batch 553.24 | loss  4.97 | ppl   144.095
| epoch   7 step     1800 |    210 batches | lr 0.01 | ms/batch 553.21 | loss  4.91 | ppl   136.167
| train loss: 4.95 | train ppl 141.316
----------------------------------------------------------------------------------------------------
| Eval   7 at step     1855 | time: 153.11s | valid loss  4.78 | valid ppl   118.757
| Total time: 1069.99s
----------------------------------------------------------------------------------------------------
| epoch   8 step     1900 |     45 batches | lr 0.01 | ms/batch 618.22 | loss  2.22 | ppl     9.252
| epoch   8 step     2000 |    145 batches | lr 0.01 | ms/batch 553.24 | loss  4.92 | ppl   136.901
| epoch   8 step     2100 |    245 batches | lr 0.01 | ms/batch 553.07 | loss  4.85 | ppl   127.963
| train loss: 4.90 | train ppl 134.570
----------------------------------------------------------------------------------------------------
| Eval   8 at step     2120 | time: 153.11s | valid loss  4.74 | valid ppl   114.952
| Total time: 1223.06s
----------------------------------------------------------------------------------------------------
| epoch   9 step     2200 |     80 batches | lr 0.01 | ms/batch 618.27 | loss  3.91 | ppl    49.965
| epoch   9 step     2300 |    180 batches | lr 0.01 | ms/batch 553.16 | loss  4.85 | ppl   127.122
| train loss: 4.86 | train ppl 128.539
----------------------------------------------------------------------------------------------------
| Eval   9 at step     2385 | time: 153.08s | valid loss  4.72 | valid ppl   111.830
| Total time: 1376.09s
----------------------------------------------------------------------------------------------------
| epoch  10 step     2400 |     15 batches | lr 0.01 | ms/batch 618.36 | loss  0.74 | ppl     2.100
| epoch  10 step     2500 |    115 batches | lr 0.01 | ms/batch 553.13 | loss  4.84 | ppl   126.841
| epoch  10 step     2600 |    215 batches | lr 0.01 | ms/batch 553.14 | loss  4.78 | ppl   118.673
| train loss: 4.82 | train ppl 124.168
----------------------------------------------------------------------------------------------------
| Eval  10 at step     2650 | time: 153.11s | valid loss  4.70 | valid ppl   110.456
| Total time: 1529.16s
----------------------------------------------------------------------------------------------------
| epoch  11 step     2700 |     50 batches | lr 0.01 | ms/batch 618.36 | loss  2.41 | ppl    11.132
| epoch  11 step     2800 |    150 batches | lr 0.01 | ms/batch 553.11 | loss  4.81 | ppl   122.844
| epoch  11 step     2900 |    250 batches | lr 0.01 | ms/batch 553.25 | loss  4.75 | ppl   115.405
| train loss: 4.79 | train ppl 120.418
----------------------------------------------------------------------------------------------------
| Eval  11 at step     2915 | time: 153.11s | valid loss  4.68 | valid ppl   107.399
| Total time: 1682.21s
----------------------------------------------------------------------------------------------------
| epoch  12 step     3000 |     85 batches | lr 0.01 | ms/batch 618.38 | loss  4.08 | ppl    58.950
| epoch  12 step     3100 |    185 batches | lr 0.01 | ms/batch 553.17 | loss  4.76 | ppl   116.883
| train loss: 4.78 | train ppl 118.543
----------------------------------------------------------------------------------------------------
| Eval  12 at step     3180 | time: 153.11s | valid loss  4.67 | valid ppl   106.698
| Total time: 1835.27s
----------------------------------------------------------------------------------------------------
| epoch  13 step     3200 |     20 batches | lr 0.01 | ms/batch 618.18 | loss  0.96 | ppl     2.615
| epoch  13 step     3300 |    120 batches | lr 0.01 | ms/batch 553.22 | loss  4.77 | ppl   117.797
| epoch  13 step     3400 |    220 batches | lr 0.01 | ms/batch 553.15 | loss  4.69 | ppl   108.637
| train loss: 4.74 | train ppl 114.900
----------------------------------------------------------------------------------------------------
| Eval  13 at step     3445 | time: 153.10s | valid loss  4.66 | valid ppl   105.970
| Total time: 1988.33s
----------------------------------------------------------------------------------------------------
| epoch  14 step     3500 |     55 batches | lr 0.01 | ms/batch 618.25 | loss  2.61 | ppl    13.582
| epoch  14 step     3600 |    155 batches | lr 0.01 | ms/batch 553.19 | loss  4.75 | ppl   115.165
| epoch  14 step     3700 |    255 batches | lr 0.01 | ms/batch 553.13 | loss  4.70 | ppl   109.585
| train loss: 4.73 | train ppl 113.159
----------------------------------------------------------------------------------------------------
| Eval  14 at step     3710 | time: 153.10s | valid loss  4.65 | valid ppl   104.835
| Total time: 2141.38s
----------------------------------------------------------------------------------------------------
| epoch  15 step     3800 |     90 batches | lr 0.01 | ms/batch 618.28 | loss  4.26 | ppl    70.953
| epoch  15 step     3900 |    190 batches | lr 0.01 | ms/batch 553.17 | loss  4.68 | ppl   107.667
| train loss: 4.71 | train ppl 110.998
----------------------------------------------------------------------------------------------------
| Eval  15 at step     3975 | time: 153.11s | valid loss  4.62 | valid ppl   101.802
| Total time: 2294.45s
----------------------------------------------------------------------------------------------------
| epoch  16 step     4000 |     25 batches | lr 0.01 | ms/batch 618.45 | loss  1.18 | ppl     3.261
| epoch  16 step     4100 |    125 batches | lr 0.01 | ms/batch 553.09 | loss  4.72 | ppl   111.637
| epoch  16 step     4200 |    225 batches | lr 0.01 | ms/batch 553.04 | loss  4.64 | ppl   103.474
| train loss: 4.69 | train ppl 108.807
----------------------------------------------------------------------------------------------------
| Eval  16 at step     4240 | time: 153.09s | valid loss  4.61 | valid ppl   100.180
| Total time: 2447.49s
----------------------------------------------------------------------------------------------------
| epoch  17 step     4300 |     60 batches | lr 0.01 | ms/batch 618.24 | loss  2.82 | ppl    16.729
| epoch  17 step     4400 |    160 batches | lr 0.01 | ms/batch 553.30 | loss  4.70 | ppl   109.832
| epoch  17 step     4500 |    260 batches | lr 0.01 | ms/batch 553.25 | loss  4.65 | ppl   104.772
| train loss: 4.68 | train ppl 107.929
----------------------------------------------------------------------------------------------------
| Eval  17 at step     4505 | time: 153.12s | valid loss  4.60 | valid ppl    99.508
| Total time: 2600.56s
----------------------------------------------------------------------------------------------------
| epoch  18 step     4600 |     95 batches | lr 0.01 | ms/batch 618.24 | loss  4.46 | ppl    86.518
| epoch  18 step     4700 |    195 batches | lr 0.01 | ms/batch 553.17 | loss  4.64 | ppl   103.028
| train loss: 4.67 | train ppl 106.596
----------------------------------------------------------------------------------------------------
| Eval  18 at step     4770 | time: 153.11s | valid loss  4.60 | valid ppl    99.409
| Total time: 2753.63s
----------------------------------------------------------------------------------------------------
| epoch  19 step     4800 |     30 batches | lr 0.01 | ms/batch 618.21 | loss  1.41 | ppl     4.085
| epoch  19 step     4900 |    130 batches | lr 0.01 | ms/batch 553.08 | loss  4.68 | ppl   107.710
| epoch  19 step     5000 |    230 batches | lr 0.01 | ms/batch 553.12 | loss  4.62 | ppl   101.051
| train loss: 4.66 | train ppl 105.687
!! Converted Model !!
----------------------------------------------------------------------------------------------------
| Eval  19 at step     5035 | time: 308.93s | valid loss  4.59 | valid ppl    98.683
| Total time: 3062.53s
----------------------------------------------------------------------------------------------------
| epoch  20 step     5100 |     65 batches | lr 0.01 | ms/batch 3787.22 | loss  3.02 | ppl    20.520
| epoch  20 step     5200 |    165 batches | lr 0.01 | ms/batch 3021.89 | loss  4.86 | ppl   129.669
| epoch  20 step     5300 |    265 batches | lr 0.01 | ms/batch 2994.99 | loss  4.67 | ppl   106.536
| train loss: 4.74 | train ppl 114.623
----------------------------------------------------------------------------------------------------
| Eval  20 at step     5300 | time: 876.91s | valid loss  4.62 | valid ppl   101.431
| Total time: 3939.63s
----------------------------------------------------------------------------------------------------
| epoch  21 step     5400 |    100 batches | lr 0.01 | ms/batch 3785.77 | loss  4.71 | ppl   111.290
| epoch  21 step     5500 |    200 batches | lr 0.01 | ms/batch 3018.48 | loss  4.64 | ppl   103.815
| train loss: 4.68 | train ppl 108.132
----------------------------------------------------------------------------------------------------
| Eval  21 at step     5565 | time: 873.98s | valid loss  4.60 | valid ppl    99.865
| Total time: 4813.90s
----------------------------------------------------------------------------------------------------
| epoch  22 step     5600 |     35 batches | lr 0.01 | ms/batch 3759.58 | loss  1.65 | ppl     5.203
| epoch  22 step     5700 |    135 batches | lr 0.01 | ms/batch 3018.95 | loss  4.68 | ppl   107.833
| epoch  22 step     5800 |    235 batches | lr 0.01 | ms/batch 3018.77 | loss  4.63 | ppl   102.024
| train loss: 4.67 | train ppl 106.455
----------------------------------------------------------------------------------------------------
| Eval  22 at step     5830 | time: 874.00s | valid loss  4.61 | valid ppl   100.028
| Total time: 5688.17s
----------------------------------------------------------------------------------------------------
| epoch  23 step     5900 |     70 batches | lr 0.01 | ms/batch 3758.65 | loss  3.26 | ppl    26.086
| epoch  23 step     6000 |    170 batches | lr 0.01 | ms/batch 3019.48 | loss  4.69 | ppl   108.567
| train loss: 4.66 | train ppl 105.467
----------------------------------------------------------------------------------------------------
| Eval  23 at step     6095 | time: 874.17s | valid loss  4.62 | valid ppl   101.441
| Total time: 6562.61s
----------------------------------------------------------------------------------------------------
| epoch  24 step     6100 |      5 batches | lr 0.01 | ms/batch 3759.88 | loss  0.24 | ppl     1.276
| epoch  24 step     6200 |    105 batches | lr 0.01 | ms/batch 3019.88 | loss  4.67 | ppl   106.375
| epoch  24 step     6300 |    205 batches | lr 0.01 | ms/batch 3019.96 | loss  4.63 | ppl   102.047
| train loss: 4.66 | train ppl 105.277
----------------------------------------------------------------------------------------------------
| Eval  24 at step     6360 | time: 874.46s | valid loss  4.61 | valid ppl   100.170
| Total time: 7437.35s
----------------------------------------------------------------------------------------------------
| epoch  25 step     6400 |     40 batches | lr 0.01 | ms/batch 3761.44 | loss  1.88 | ppl     6.545
| epoch  25 step     6500 |    140 batches | lr 0.01 | ms/batch 3019.98 | loss  4.66 | ppl   105.403
| epoch  25 step     6600 |    240 batches | lr 0.01 | ms/batch 3020.11 | loss  4.61 | ppl   100.428
| train loss: 4.65 | train ppl 104.472
----------------------------------------------------------------------------------------------------
| Eval  25 at step     6625 | time: 874.35s | valid loss  4.61 | valid ppl   100.813
| Total time: 8311.97s
----------------------------------------------------------------------------------------------------
| epoch  26 step     6700 |     75 batches | lr 0.01 | ms/batch 3760.59 | loss  3.49 | ppl    32.807
| epoch  26 step     6800 |    175 batches | lr 0.01 | ms/batch 3019.77 | loss  4.65 | ppl   104.840
| train loss: 4.64 | train ppl 103.822
----------------------------------------------------------------------------------------------------
| Eval  26 at step     6890 | time: 874.19s | valid loss  4.61 | valid ppl   100.122
| Total time: 9186.44s
----------------------------------------------------------------------------------------------------
| epoch  27 step     6900 |     10 batches | lr 0.01 | ms/batch 3759.20 | loss  0.48 | ppl     1.614
| epoch  27 step     7000 |    110 batches | lr 0.01 | ms/batch 3019.31 | loss  4.64 | ppl   103.204
| epoch  27 step     7100 |    210 batches | lr 0.01 | ms/batch 3019.52 | loss  4.60 | ppl    99.017
| train loss: 4.63 | train ppl 102.873
----------------------------------------------------------------------------------------------------
| Eval  27 at step     7155 | time: 874.25s | valid loss  4.61 | valid ppl   100.602
| Total time: 10060.97s
----------------------------------------------------------------------------------------------------
| epoch  28 step     7200 |     45 batches | lr 0.001 | ms/batch 3760.72 | loss  2.11 | ppl     8.233
| epoch  28 step     7300 |    145 batches | lr 0.001 | ms/batch 3020.45 | loss  4.58 | ppl    97.149
| epoch  28 step     7400 |    245 batches | lr 0.001 | ms/batch 3022.27 | loss  4.42 | ppl    83.188
| train loss: 4.53 | train ppl 92.703
----------------------------------------------------------------------------------------------------
| Eval  28 at step     7420 | time: 874.66s | valid loss  4.55 | valid ppl    94.841
| Total time: 10935.92s
----------------------------------------------------------------------------------------------------
| epoch  29 step     7500 |     80 batches | lr 0.001 | ms/batch 3774.67 | loss  3.67 | ppl    39.081
| epoch  29 step     7600 |    180 batches | lr 0.001 | ms/batch 3019.29 | loss  4.49 | ppl    89.102
| train loss: 4.49 | train ppl 88.980
----------------------------------------------------------------------------------------------------
| Eval  29 at step     7685 | time: 875.62s | valid loss  4.54 | valid ppl    93.474
| Total time: 11811.81s
----------------------------------------------------------------------------------------------------
| epoch  30 step     7700 |     15 batches | lr 0.001 | ms/batch 3775.39 | loss  0.70 | ppl     2.012
| epoch  30 step     7800 |    115 batches | lr 0.001 | ms/batch 3018.82 | loss  4.54 | ppl    93.296
| epoch  30 step     7900 |    215 batches | lr 0.001 | ms/batch 3019.12 | loss  4.38 | ppl    80.161
| train loss: 4.46 | train ppl 86.614
----------------------------------------------------------------------------------------------------
| Eval  30 at step     7950 | time: 875.58s | valid loss  4.53 | valid ppl    92.502
| Total time: 12687.59s
----------------------------------------------------------------------------------------------------
| epoch  31 step     8000 |     50 batches | lr 0.001 | ms/batch 3774.72 | loss  2.26 | ppl     9.588
| epoch  31 step     8100 |    150 batches | lr 0.001 | ms/batch 3023.29 | loss  4.47 | ppl    87.530
| epoch  31 step     8200 |    250 batches | lr 0.001 | ms/batch 3029.61 | loss  4.36 | ppl    77.986
| train loss: 4.44 | train ppl 84.597
----------------------------------------------------------------------------------------------------
| Eval  31 at step     8215 | time: 877.35s | valid loss  4.52 | valid ppl    92.050
| Total time: 13565.09s
----------------------------------------------------------------------------------------------------
| epoch  32 step     8300 |     85 batches | lr 0.001 | ms/batch 3785.81 | loss  3.82 | ppl    45.750
| epoch  32 step     8400 |    185 batches | lr 0.001 | ms/batch 3027.03 | loss  4.42 | ppl    82.901
| train loss: 4.43 | train ppl 84.125
----------------------------------------------------------------------------------------------------
| Eval  32 at step     8480 | time: 877.56s | valid loss  4.51 | valid ppl    91.301
| Total time: 14442.82s
----------------------------------------------------------------------------------------------------
| epoch  33 step     8500 |     20 batches | lr 0.001 | ms/batch 3777.25 | loss  0.91 | ppl     2.482
| epoch  33 step     8600 |    120 batches | lr 0.001 | ms/batch 3029.46 | loss  4.46 | ppl    86.103
| epoch  33 step     8700 |    220 batches | lr 0.001 | ms/batch 3021.61 | loss  4.33 | ppl    76.278
| train loss: 4.41 | train ppl 81.992
----------------------------------------------------------------------------------------------------
| Eval  33 at step     8745 | time: 877.16s | valid loss  4.51 | valid ppl    90.639
| Total time: 15320.15s
----------------------------------------------------------------------------------------------------
| epoch  34 step     8800 |     55 batches | lr 0.001 | ms/batch 3778.83 | loss  2.44 | ppl    11.467
| epoch  34 step     8900 |    155 batches | lr 0.001 | ms/batch 3021.52 | loss  4.44 | ppl    84.569
| epoch  34 step     9000 |    255 batches | lr 0.001 | ms/batch 3021.57 | loss  4.33 | ppl    75.967
| train loss: 4.40 | train ppl 81.163
----------------------------------------------------------------------------------------------------
| Eval  34 at step     9010 | time: 876.23s | valid loss  4.51 | valid ppl    90.468
| Total time: 16196.56s
----------------------------------------------------------------------------------------------------
====================================================================================================
| End of training | test loss  4.46 | test ppl    86.389
| Maximum memory usage: 5568.72MB
====================================================================================================
