|Basic Memory Usage: 8.69MB current, 8.69MB max
====================================================================================================
    - data : ../data/penn/
    - dataset : ptb
    - n_layer : 20
    - d_embed : 400
    - nhid : 1000
    - nout : 400
    - epochs : 500
    - halt_ppl : None
    - time_limit : 16000.0
    - optim : Adam
    - lr : 0.01
    - clip : 0.225
    - batch_size : 48
    - batch_chunk : 1
    - seq_len : 110
    - subseq_len : 55
    - dropout : 0.45
    - dropouti : 0.45
    - wdrop : 0.5
    - emb_dropout : 0.1
    - dropouth : 0.28
    - weight_decay : 1.2e-06
    - wnorm : True
    - seed : 500
    - cuda : True
    - not_tied : False
    - anneal : 10
    - log_interval : 100
    - force_deq_validation : False
    - when : [-1]
    - ksize : 2
    - dilation : 1
    - n_experts : 0
    - multi_gpu : False
    - use_gpus : None
    - f_thres : 45
    - b_thres : 45
    - work_dir : LM-TREdeq-ptb/mem_sat_20kl_20210602-114458
    - restart : False
    - restart_dir : 
    - debug : False
    - gpu0_bsz : -1
    - pretrain_steps : 5000
    - start_train_steps : 0
    - timing : True
    - eval_mem : False
    - eval : False
    - load : 
    - name : mem_sat_20kl
    - tied : True
    - n_token : 10000
    - n_all_param : 24193603
====================================================================================================
#params = 24193603
|Static Memory Usage: 252.25MB current, 269.03MB max
| epoch   1 step      100 |    100 batches | lr 0.01 | ms/batch 852.00 | loss  6.34 | ppl   565.945
| train loss: 6.09 | train ppl 443.229
----------------------------------------------------------------------------------------------------
| Eval   1 at step      177 | time: 153.33s | valid loss  5.48 | valid ppl   239.958
| Total time: 150.85s
----------------------------------------------------------------------------------------------------
| epoch   2 step      200 |     23 batches | lr 0.01 | ms/batch 884.07 | loss  1.31 | ppl     3.718
| epoch   2 step      300 |    123 batches | lr 0.01 | ms/batch 823.42 | loss  5.56 | ppl   259.888
| train loss: 5.54 | train ppl 253.749
----------------------------------------------------------------------------------------------------
| Eval   2 at step      354 | time: 151.84s | valid loss  5.19 | valid ppl   180.215
| Total time: 302.77s
----------------------------------------------------------------------------------------------------
| epoch   3 step      400 |     46 batches | lr 0.01 | ms/batch 885.90 | loss  2.49 | ppl    12.019
| epoch   3 step      500 |    146 batches | lr 0.01 | ms/batch 824.36 | loss  5.30 | ppl   200.098
| train loss: 5.32 | train ppl 204.945
----------------------------------------------------------------------------------------------------
| Eval   3 at step      531 | time: 152.09s | valid loss  5.04 | valid ppl   155.017
| Total time: 454.89s
----------------------------------------------------------------------------------------------------
| epoch   4 step      600 |     69 batches | lr 0.01 | ms/batch 887.17 | loss  3.62 | ppl    37.483
| epoch   4 step      700 |    169 batches | lr 0.01 | ms/batch 824.27 | loss  5.16 | ppl   174.096
| train loss: 5.20 | train ppl 180.630
----------------------------------------------------------------------------------------------------
| Eval   4 at step      708 | time: 152.20s | valid loss  4.95 | valid ppl   140.750
| Total time: 607.14s
----------------------------------------------------------------------------------------------------
| epoch   5 step      800 |     92 batches | lr 0.01 | ms/batch 887.32 | loss  4.74 | ppl   114.301
| train loss: 5.09 | train ppl 163.001
----------------------------------------------------------------------------------------------------
| Eval   5 at step      885 | time: 152.19s | valid loss  4.87 | valid ppl   129.981
| Total time: 759.37s
----------------------------------------------------------------------------------------------------
| epoch   6 step      900 |     15 batches | lr 0.01 | ms/batch 886.72 | loss  0.76 | ppl     2.145
| epoch   6 step     1000 |    115 batches | lr 0.01 | ms/batch 823.71 | loss  5.03 | ppl   153.320
| train loss: 5.02 | train ppl 150.875
----------------------------------------------------------------------------------------------------
| Eval   6 at step     1062 | time: 152.09s | valid loss  4.82 | valid ppl   123.629
| Total time: 911.50s
----------------------------------------------------------------------------------------------------
| epoch   7 step     1100 |     38 batches | lr 0.01 | ms/batch 887.83 | loss  1.90 | ppl     6.665
| epoch   7 step     1200 |    138 batches | lr 0.01 | ms/batch 824.45 | loss  4.93 | ppl   138.592
| train loss: 4.94 | train ppl 139.792
----------------------------------------------------------------------------------------------------
| Eval   7 at step     1239 | time: 152.27s | valid loss  4.78 | valid ppl   119.339
| Total time: 1063.81s
----------------------------------------------------------------------------------------------------
| epoch   8 step     1300 |     61 batches | lr 0.01 | ms/batch 887.50 | loss  3.00 | ppl    20.166
| epoch   8 step     1400 |    161 batches | lr 0.01 | ms/batch 824.69 | loss  4.89 | ppl   132.380
| train loss: 4.90 | train ppl 134.277
----------------------------------------------------------------------------------------------------
| Eval   8 at step     1416 | time: 152.23s | valid loss  4.76 | valid ppl   116.694
| Total time: 1216.10s
----------------------------------------------------------------------------------------------------
| epoch   9 step     1500 |     84 batches | lr 0.01 | ms/batch 887.42 | loss  4.11 | ppl    60.782
| train loss: 4.85 | train ppl 128.016
----------------------------------------------------------------------------------------------------
| Eval   9 at step     1593 | time: 152.23s | valid loss  4.73 | valid ppl   113.298
| Total time: 1368.38s
----------------------------------------------------------------------------------------------------
| epoch  10 step     1600 |      7 batches | lr 0.01 | ms/batch 887.34 | loss  0.34 | ppl     1.405
| epoch  10 step     1700 |    107 batches | lr 0.01 | ms/batch 824.52 | loss  4.83 | ppl   124.757
| train loss: 4.81 | train ppl 123.040
----------------------------------------------------------------------------------------------------
| Eval  10 at step     1770 | time: 152.20s | valid loss  4.71 | valid ppl   111.349
| Total time: 1520.63s
----------------------------------------------------------------------------------------------------
| epoch  11 step     1800 |     30 batches | lr 0.01 | ms/batch 887.44 | loss  1.45 | ppl     4.258
| epoch  11 step     1900 |    130 batches | lr 0.01 | ms/batch 824.46 | loss  4.77 | ppl   117.757
| train loss: 4.77 | train ppl 118.240
----------------------------------------------------------------------------------------------------
| Eval  11 at step     1947 | time: 152.25s | valid loss  4.70 | valid ppl   109.982
| Total time: 1672.92s
----------------------------------------------------------------------------------------------------
| epoch  12 step     2000 |     53 batches | lr 0.01 | ms/batch 887.23 | loss  2.53 | ppl    12.600
| epoch  12 step     2100 |    153 batches | lr 0.01 | ms/batch 824.69 | loss  4.74 | ppl   114.293
| train loss: 4.75 | train ppl 115.490
----------------------------------------------------------------------------------------------------
| Eval  12 at step     2124 | time: 152.22s | valid loss  4.68 | valid ppl   107.956
| Total time: 1825.19s
----------------------------------------------------------------------------------------------------
| epoch  13 step     2200 |     76 batches | lr 0.01 | ms/batch 887.15 | loss  3.61 | ppl    36.883
| epoch  13 step     2300 |    176 batches | lr 0.01 | ms/batch 824.58 | loss  4.70 | ppl   110.044
| train loss: 4.72 | train ppl 112.214
----------------------------------------------------------------------------------------------------
| Eval  13 at step     2301 | time: 152.21s | valid loss  4.64 | valid ppl   103.896
| Total time: 1977.42s
----------------------------------------------------------------------------------------------------
| epoch  14 step     2400 |     99 batches | lr 0.01 | ms/batch 887.55 | loss  4.67 | ppl   107.085
| train loss: 4.70 | train ppl 109.830
----------------------------------------------------------------------------------------------------
| Eval  14 at step     2478 | time: 152.25s | valid loss  4.63 | valid ppl   102.504
| Total time: 2129.72s
----------------------------------------------------------------------------------------------------
| epoch  15 step     2500 |     22 batches | lr 0.01 | ms/batch 887.77 | loss  1.04 | ppl     2.818
| epoch  15 step     2600 |    122 batches | lr 0.01 | ms/batch 824.80 | loss  4.68 | ppl   107.841
| train loss: 4.67 | train ppl 107.086
----------------------------------------------------------------------------------------------------
| Eval  15 at step     2655 | time: 152.32s | valid loss  4.61 | valid ppl   100.307
| Total time: 2282.08s
----------------------------------------------------------------------------------------------------
| epoch  16 step     2700 |     45 batches | lr 0.01 | ms/batch 887.86 | loss  2.11 | ppl     8.250
| epoch  16 step     2800 |    145 batches | lr 0.01 | ms/batch 824.54 | loss  4.65 | ppl   104.585
| train loss: 4.66 | train ppl 105.494
----------------------------------------------------------------------------------------------------
| Eval  16 at step     2832 | time: 152.24s | valid loss  4.60 | valid ppl    99.819
| Total time: 2434.37s
----------------------------------------------------------------------------------------------------
| epoch  17 step     2900 |     68 batches | lr 0.01 | ms/batch 887.42 | loss  3.17 | ppl    23.904
| epoch  17 step     3000 |    168 batches | lr 0.01 | ms/batch 824.52 | loss  4.63 | ppl   102.135
| train loss: 4.64 | train ppl 103.817
----------------------------------------------------------------------------------------------------
| Eval  17 at step     3009 | time: 152.26s | valid loss  4.60 | valid ppl    99.420
| Total time: 2586.68s
----------------------------------------------------------------------------------------------------
| epoch  18 step     3100 |     91 batches | lr 0.01 | ms/batch 888.33 | loss  4.25 | ppl    69.929
| train loss: 4.62 | train ppl 101.954
----------------------------------------------------------------------------------------------------
| Eval  18 at step     3186 | time: 152.27s | valid loss  4.60 | valid ppl    99.181
| Total time: 2739.00s
----------------------------------------------------------------------------------------------------
| epoch  19 step     3200 |     14 batches | lr 0.01 | ms/batch 887.36 | loss  0.65 | ppl     1.919
| epoch  19 step     3300 |    114 batches | lr 0.01 | ms/batch 824.85 | loss  4.62 | ppl   101.582
| train loss: 4.62 | train ppl 101.171
----------------------------------------------------------------------------------------------------
| Eval  19 at step     3363 | time: 152.31s | valid loss  4.60 | valid ppl    99.254
| Total time: 2891.35s
----------------------------------------------------------------------------------------------------
| epoch  20 step     3400 |     37 batches | lr 0.01 | ms/batch 872.48 | loss  1.71 | ppl     5.516
| epoch  20 step     3500 |    137 batches | lr 0.01 | ms/batch 824.87 | loss  4.59 | ppl    98.913
| train loss: 4.60 | train ppl 99.183
----------------------------------------------------------------------------------------------------
| Eval  20 at step     3540 | time: 150.76s | valid loss  4.58 | valid ppl    97.546
| Total time: 3042.26s
----------------------------------------------------------------------------------------------------
| epoch  21 step     3600 |     60 batches | lr 0.01 | ms/batch 888.17 | loss  2.76 | ppl    15.767
| epoch  21 step     3700 |    160 batches | lr 0.01 | ms/batch 824.95 | loss  4.58 | ppl    97.207
| train loss: 4.58 | train ppl 97.705
----------------------------------------------------------------------------------------------------
| Eval  21 at step     3717 | time: 152.33s | valid loss  4.59 | valid ppl    98.531
| Total time: 3194.63s
----------------------------------------------------------------------------------------------------
| epoch  22 step     3800 |     83 batches | lr 0.01 | ms/batch 872.38 | loss  3.83 | ppl    46.150
| train loss: 4.58 | train ppl 97.519
----------------------------------------------------------------------------------------------------
| Eval  22 at step     3894 | time: 150.80s | valid loss  4.58 | valid ppl    97.738
| Total time: 3345.56s
----------------------------------------------------------------------------------------------------
| epoch  23 step     3900 |      6 batches | lr 0.01 | ms/batch 872.64 | loss  0.27 | ppl     1.316
| epoch  23 step     4000 |    106 batches | lr 0.01 | ms/batch 824.93 | loss  4.57 | ppl    96.990
| train loss: 4.57 | train ppl 96.799
----------------------------------------------------------------------------------------------------
| Eval  23 at step     4071 | time: 150.77s | valid loss  4.56 | valid ppl    95.954
| Total time: 3496.47s
----------------------------------------------------------------------------------------------------
| epoch  24 step     4100 |     29 batches | lr 0.01 | ms/batch 887.91 | loss  1.33 | ppl     3.788
| epoch  24 step     4200 |    129 batches | lr 0.01 | ms/batch 825.17 | loss  4.57 | ppl    96.293
| train loss: 4.56 | train ppl 95.732
----------------------------------------------------------------------------------------------------
| Eval  24 at step     4248 | time: 152.34s | valid loss  4.56 | valid ppl    95.531
| Total time: 3648.85s
----------------------------------------------------------------------------------------------------
| epoch  25 step     4300 |     52 batches | lr 0.01 | ms/batch 888.04 | loss  2.38 | ppl    10.765
| epoch  25 step     4400 |    152 batches | lr 0.01 | ms/batch 825.16 | loss  4.53 | ppl    92.856
| train loss: 4.54 | train ppl 94.001
----------------------------------------------------------------------------------------------------
| Eval  25 at step     4425 | time: 152.35s | valid loss  4.55 | valid ppl    94.681
| Total time: 3801.25s
----------------------------------------------------------------------------------------------------
| epoch  26 step     4500 |     75 batches | lr 0.01 | ms/batch 887.99 | loss  3.42 | ppl    30.717
| epoch  26 step     4600 |    175 batches | lr 0.01 | ms/batch 825.49 | loss  4.51 | ppl    91.267
| train loss: 4.54 | train ppl 93.323
----------------------------------------------------------------------------------------------------
| Eval  26 at step     4602 | time: 152.36s | valid loss  4.55 | valid ppl    94.188
| Total time: 3953.66s
----------------------------------------------------------------------------------------------------
| epoch  27 step     4700 |     98 batches | lr 0.01 | ms/batch 888.25 | loss  4.47 | ppl    87.500
| train loss: 4.53 | train ppl 93.068
----------------------------------------------------------------------------------------------------
| Eval  27 at step     4779 | time: 152.38s | valid loss  4.54 | valid ppl    93.279
| Total time: 4106.10s
----------------------------------------------------------------------------------------------------
| epoch  28 step     4800 |     21 batches | lr 0.01 | ms/batch 888.36 | loss  0.96 | ppl     2.615
| epoch  28 step     4900 |    121 batches | lr 0.01 | ms/batch 825.37 | loss  4.53 | ppl    92.493
| train loss: 4.52 | train ppl 92.060
----------------------------------------------------------------------------------------------------
| Eval  28 at step     4956 | time: 152.38s | valid loss  4.54 | valid ppl    93.406
| Total time: 4258.53s
----------------------------------------------------------------------------------------------------
| epoch  29 step     5000 |     44 batches | lr 0.01 | ms/batch 872.68 | loss  2.00 | ppl     7.385
| epoch  29 step     5100 |    144 batches | lr 0.01 | ms/batch 4283.68 | loss  4.49 | ppl    89.229
| train loss: 4.51 | train ppl 90.795
!! Converted Model !!
----------------------------------------------------------------------------------------------------
| Eval  29 at step     5133 | time: 679.18s | valid loss  4.51 | valid ppl    91.369
| Total time: 4938.29s
----------------------------------------------------------------------------------------------------
| epoch  30 step     5200 |     67 batches | lr 0.01 | ms/batch 5032.50 | loss  3.02 | ppl    20.588
| epoch  30 step     5300 |    167 batches | lr 0.01 | ms/batch 4278.01 | loss  4.50 | ppl    89.945
| train loss: 4.50 | train ppl 90.264
----------------------------------------------------------------------------------------------------
| Eval  30 at step     5310 | time: 832.35s | valid loss  4.68 | valid ppl   107.892
| Total time: 5771.00s
----------------------------------------------------------------------------------------------------
| epoch  31 step     5400 |     90 batches | lr 0.001 | ms/batch 4992.63 | loss  4.29 | ppl    72.876
| train loss: 4.71 | train ppl 111.461
----------------------------------------------------------------------------------------------------
| Eval  31 at step     5487 | time: 826.39s | valid loss  5.16 | valid ppl   175.019
| Total time: 6597.89s
----------------------------------------------------------------------------------------------------
| epoch  32 step     5500 |     13 batches | lr 0.0001 | ms/batch 4981.97 | loss  0.65 | ppl     1.909
| epoch  32 step     5600 |    113 batches | lr 0.0001 | ms/batch 4257.17 | loss  4.93 | ppl   137.751
| train loss: 4.89 | train ppl 133.057
----------------------------------------------------------------------------------------------------
| Eval  32 at step     5664 | time: 826.20s | valid loss  5.15 | valid ppl   171.780
| Total time: 7424.69s
----------------------------------------------------------------------------------------------------
| epoch  33 step     5700 |     36 batches | lr 1e-05 | ms/batch 4983.03 | loss  1.78 | ppl     5.919
| epoch  33 step     5800 |    136 batches | lr 1e-05 | ms/batch 4256.46 | loss  4.88 | ppl   132.140
| train loss: 4.88 | train ppl 131.257
----------------------------------------------------------------------------------------------------
| Eval  33 at step     5841 | time: 826.69s | valid loss  5.15 | valid ppl   171.774
| Total time: 8251.91s
----------------------------------------------------------------------------------------------------
| epoch  34 step     5900 |     59 batches | lr 1e-05 | ms/batch 4994.65 | loss  2.92 | ppl    18.500
| epoch  34 step     6000 |    159 batches | lr 1e-05 | ms/batch 4263.04 | loss  4.85 | ppl   127.644
| train loss: 4.88 | train ppl 131.598
----------------------------------------------------------------------------------------------------
| Eval  34 at step     6018 | time: 827.61s | valid loss  5.14 | valid ppl   171.379
| Total time: 9080.31s
----------------------------------------------------------------------------------------------------
| epoch  35 step     6100 |     82 batches | lr 1e-05 | ms/batch 4993.33 | loss  4.06 | ppl    57.836
| train loss: 4.88 | train ppl 131.988
----------------------------------------------------------------------------------------------------
| Eval  35 at step     6195 | time: 827.69s | valid loss  5.14 | valid ppl   170.134
| Total time: 9908.67s
----------------------------------------------------------------------------------------------------
| epoch  36 step     6200 |      5 batches | lr 1e-05 | ms/batch 4994.37 | loss  0.25 | ppl     1.284
| epoch  36 step     6300 |    105 batches | lr 1e-05 | ms/batch 4261.24 | loss  4.92 | ppl   137.537
| train loss: 4.88 | train ppl 131.861
----------------------------------------------------------------------------------------------------
| Eval  36 at step     6372 | time: 826.54s | valid loss  5.14 | valid ppl   170.105
| Total time: 10735.62s
----------------------------------------------------------------------------------------------------
| epoch  37 step     6400 |     28 batches | lr 1e-05 | ms/batch 4981.86 | loss  1.39 | ppl     4.030
| epoch  37 step     6500 |    128 batches | lr 1e-05 | ms/batch 4252.98 | loss  4.89 | ppl   132.962
| train loss: 4.88 | train ppl 131.022
----------------------------------------------------------------------------------------------------
| Eval  37 at step     6549 | time: 825.49s | valid loss  5.10 | valid ppl   164.699
| Total time: 11561.83s
----------------------------------------------------------------------------------------------------
| epoch  38 step     6600 |     51 batches | lr 1e-05 | ms/batch 4980.64 | loss  2.52 | ppl    12.451
| epoch  38 step     6700 |    151 batches | lr 1e-05 | ms/batch 4253.83 | loss  4.86 | ppl   129.242
| train loss: 4.88 | train ppl 131.767
----------------------------------------------------------------------------------------------------
| Eval  38 at step     6726 | time: 825.64s | valid loss  5.07 | valid ppl   158.721
| Total time: 12388.19s
----------------------------------------------------------------------------------------------------
| epoch  39 step     6800 |     74 batches | lr 1e-05 | ms/batch 4981.34 | loss  3.67 | ppl    39.114
| epoch  39 step     6900 |    174 batches | lr 1e-05 | ms/batch 4254.02 | loss  4.84 | ppl   125.984
| train loss: 4.89 | train ppl 132.520
----------------------------------------------------------------------------------------------------
| Eval  39 at step     6903 | time: 825.70s | valid loss  5.02 | valid ppl   151.664
| Total time: 13214.61s
----------------------------------------------------------------------------------------------------
| epoch  40 step     7000 |     97 batches | lr 1e-05 | ms/batch 4981.66 | loss  4.79 | ppl   120.615
| train loss: 4.88 | train ppl 131.732
----------------------------------------------------------------------------------------------------
| Eval  40 at step     7080 | time: 825.74s | valid loss  4.96 | valid ppl   143.258
| Total time: 14041.06s
----------------------------------------------------------------------------------------------------
| epoch  41 step     7100 |     20 batches | lr 1e-05 | ms/batch 4981.28 | loss  0.99 | ppl     2.699
| epoch  41 step     7200 |    120 batches | lr 1e-05 | ms/batch 4252.61 | loss  4.91 | ppl   135.422
| train loss: 4.88 | train ppl 131.465
----------------------------------------------------------------------------------------------------
| Eval  41 at step     7257 | time: 825.49s | valid loss  4.86 | valid ppl   128.683
| Total time: 14867.16s
----------------------------------------------------------------------------------------------------
| epoch  42 step     7300 |     43 batches | lr 1e-05 | ms/batch 4980.74 | loss  2.13 | ppl     8.396
| epoch  42 step     7400 |    143 batches | lr 1e-05 | ms/batch 4254.90 | loss  4.87 | ppl   129.921
| train loss: 4.88 | train ppl 131.181
----------------------------------------------------------------------------------------------------
| Eval  42 at step     7434 | time: 825.80s | valid loss  4.77 | valid ppl   117.336
| Total time: 15693.65s
----------------------------------------------------------------------------------------------------
| epoch  43 step     7500 |     66 batches | lr 1e-05 | ms/batch 4981.49 | loss  3.25 | ppl    25.900
| epoch  43 step     7600 |    166 batches | lr 1e-05 | ms/batch 4253.40 | loss  4.85 | ppl   127.498
| train loss: 4.88 | train ppl 131.448
----------------------------------------------------------------------------------------------------
| Eval  43 at step     7611 | time: 825.60s | valid loss  4.70 | valid ppl   110.331
| Total time: 16519.94s
----------------------------------------------------------------------------------------------------
====================================================================================================
| End of training | test loss  4.48 | test ppl    88.413
| Maximum memory usage: 7994.64MB
====================================================================================================
