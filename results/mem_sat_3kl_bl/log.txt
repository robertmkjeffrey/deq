|Basic Memory Usage: 8.69MB current, 8.69MB max
====================================================================================================
    - data : ../data/penn/
    - dataset : ptb
    - n_layer : 3
    - d_embed : 400
    - nhid : 1000
    - nout : 400
    - epochs : 500
    - halt_ppl : None
    - time_limit : 16000.0
    - optim : Adam
    - lr : 0.01
    - clip : 0.225
    - batch_size : 32
    - batch_chunk : 1
    - seq_len : 110
    - subseq_len : 55
    - dropout : 0.45
    - dropouti : 0.45
    - wdrop : 0.5
    - emb_dropout : 0.1
    - dropouth : 0.28
    - weight_decay : 1.2e-06
    - wnorm : True
    - seed : 500
    - cuda : True
    - not_tied : False
    - anneal : 10
    - log_interval : 100
    - force_deq_validation : False
    - when : [-1]
    - ksize : 2
    - dilation : 1
    - n_experts : 0
    - multi_gpu : False
    - use_gpus : None
    - f_thres : 45
    - b_thres : 45
    - work_dir : LM-TREdeq-ptb/mem_sat_3kl_20210602-165259
    - restart : False
    - restart_dir : 
    - debug : False
    - gpu0_bsz : -1
    - pretrain_steps : 5000
    - start_train_steps : 0
    - timing : True
    - eval_mem : False
    - eval : False
    - load : 
    - name : mem_sat_3kl
    - tied : True
    - n_token : 10000
    - n_all_param : 24193603
====================================================================================================
#params = 24193603
|Static Memory Usage: 252.25MB current, 269.03MB max
| epoch   1 step      100 |    100 batches | lr 0.01 | ms/batch 109.08 | loss  6.37 | ppl   586.729
| epoch   1 step      200 |    200 batches | lr 0.01 | ms/batch 95.12 | loss  5.76 | ppl   316.327
| train loss: 5.96 | train ppl 388.936
----------------------------------------------------------------------------------------------------
| Eval   1 at step      265 | time: 27.87s | valid loss  5.36 | valid ppl   212.486
| Total time: 26.81s
----------------------------------------------------------------------------------------------------
| epoch   2 step      300 |     35 batches | lr 0.01 | ms/batch 122.08 | loss  1.96 | ppl     7.101
| epoch   2 step      400 |    135 batches | lr 0.01 | ms/batch 95.16 | loss  5.49 | ppl   242.074
| epoch   2 step      500 |    235 batches | lr 0.01 | ms/batch 95.19 | loss  5.37 | ppl   215.251
| train loss: 5.45 | train ppl 232.876
----------------------------------------------------------------------------------------------------
| Eval   2 at step      530 | time: 27.91s | valid loss  5.13 | valid ppl   169.446
| Total time: 54.67s
----------------------------------------------------------------------------------------------------
| epoch   3 step      600 |     70 batches | lr 0.01 | ms/batch 122.78 | loss  3.73 | ppl    41.631
| epoch   3 step      700 |    170 batches | lr 0.01 | ms/batch 95.22 | loss  5.28 | ppl   196.258
| train loss: 5.27 | train ppl 193.984
----------------------------------------------------------------------------------------------------
| Eval   3 at step      795 | time: 27.99s | valid loss  5.01 | valid ppl   150.597
| Total time: 82.58s
----------------------------------------------------------------------------------------------------
| epoch   4 step      800 |      5 batches | lr 0.01 | ms/batch 122.95 | loss  0.27 | ppl     1.310
| epoch   4 step      900 |    105 batches | lr 0.01 | ms/batch 95.15 | loss  5.20 | ppl   182.008
| epoch   4 step     1000 |    205 batches | lr 0.01 | ms/batch 95.17 | loss  5.13 | ppl   169.496
| train loss: 5.17 | train ppl 176.117
----------------------------------------------------------------------------------------------------
| Eval   4 at step     1060 | time: 27.99s | valid loss  4.95 | valid ppl   141.596
| Total time: 110.48s
----------------------------------------------------------------------------------------------------
| epoch   5 step     1100 |     40 batches | lr 0.01 | ms/batch 122.86 | loss  2.06 | ppl     7.842
| epoch   5 step     1200 |    140 batches | lr 0.01 | ms/batch 95.17 | loss  5.11 | ppl   165.333
| epoch   5 step     1300 |    240 batches | lr 0.01 | ms/batch 95.16 | loss  5.05 | ppl   155.352
| train loss: 5.09 | train ppl 162.826
----------------------------------------------------------------------------------------------------
| Eval   5 at step     1325 | time: 27.99s | valid loss  4.91 | valid ppl   135.955
| Total time: 138.38s
----------------------------------------------------------------------------------------------------
| epoch   6 step     1400 |     75 batches | lr 0.01 | ms/batch 122.68 | loss  3.81 | ppl    45.075
| epoch   6 step     1500 |    175 batches | lr 0.01 | ms/batch 95.16 | loss  5.04 | ppl   154.083
| train loss: 5.04 | train ppl 154.309
----------------------------------------------------------------------------------------------------
| Eval   6 at step     1590 | time: 27.96s | valid loss  4.88 | valid ppl   131.307
| Total time: 166.25s
----------------------------------------------------------------------------------------------------
| epoch   7 step     1600 |     10 batches | lr 0.01 | ms/batch 122.69 | loss  0.52 | ppl     1.674
| epoch   7 step     1700 |    110 batches | lr 0.01 | ms/batch 95.18 | loss  5.02 | ppl   151.263
| epoch   7 step     1800 |    210 batches | lr 0.01 | ms/batch 95.13 | loss  4.97 | ppl   143.862
| train loss: 5.00 | train ppl 148.609
----------------------------------------------------------------------------------------------------
| Eval   7 at step     1855 | time: 28.20s | valid loss  4.86 | valid ppl   129.654
| Total time: 194.36s
----------------------------------------------------------------------------------------------------
| epoch   8 step     1900 |     45 batches | lr 0.01 | ms/batch 125.07 | loss  2.25 | ppl     9.481
| epoch   8 step     2000 |    145 batches | lr 0.01 | ms/batch 95.23 | loss  4.99 | ppl   146.262
| epoch   8 step     2100 |    245 batches | lr 0.01 | ms/batch 95.18 | loss  4.92 | ppl   137.041
| train loss: 4.97 | train ppl 143.618
----------------------------------------------------------------------------------------------------
| Eval   8 at step     2120 | time: 27.99s | valid loss  4.85 | valid ppl   127.311
| Total time: 222.27s
----------------------------------------------------------------------------------------------------
| epoch   9 step     2200 |     80 batches | lr 0.01 | ms/batch 122.94 | loss  3.97 | ppl    52.819
| epoch   9 step     2300 |    180 batches | lr 0.01 | ms/batch 95.18 | loss  4.93 | ppl   138.948
| train loss: 4.94 | train ppl 139.192
----------------------------------------------------------------------------------------------------
| Eval   9 at step     2385 | time: 28.00s | valid loss  4.83 | valid ppl   125.553
| Total time: 250.18s
----------------------------------------------------------------------------------------------------
| epoch  10 step     2400 |     15 batches | lr 0.01 | ms/batch 122.92 | loss  0.75 | ppl     2.123
| epoch  10 step     2500 |    115 batches | lr 0.01 | ms/batch 95.16 | loss  4.93 | ppl   137.969
| epoch  10 step     2600 |    215 batches | lr 0.01 | ms/batch 95.18 | loss  4.87 | ppl   130.202
| train loss: 4.91 | train ppl 135.265
----------------------------------------------------------------------------------------------------
| Eval  10 at step     2650 | time: 28.00s | valid loss  4.82 | valid ppl   123.785
| Total time: 278.09s
----------------------------------------------------------------------------------------------------
| epoch  11 step     2700 |     50 batches | lr 0.01 | ms/batch 122.73 | loss  2.46 | ppl    11.651
| epoch  11 step     2800 |    150 batches | lr 0.01 | ms/batch 95.16 | loss  4.91 | ppl   135.878
| epoch  11 step     2900 |    250 batches | lr 0.01 | ms/batch 95.15 | loss  4.85 | ppl   127.880
| train loss: 4.89 | train ppl 133.077
----------------------------------------------------------------------------------------------------
| Eval  11 at step     2915 | time: 27.98s | valid loss  4.81 | valid ppl   122.245
| Total time: 305.99s
----------------------------------------------------------------------------------------------------
| epoch  12 step     3000 |     85 batches | lr 0.01 | ms/batch 123.10 | loss  4.17 | ppl    64.453
| epoch  12 step     3100 |    185 batches | lr 0.01 | ms/batch 95.16 | loss  4.87 | ppl   130.718
| train loss: 4.88 | train ppl 131.956
----------------------------------------------------------------------------------------------------
| Eval  12 at step     3180 | time: 28.01s | valid loss  4.80 | valid ppl   121.308
| Total time: 333.92s
----------------------------------------------------------------------------------------------------
| epoch  13 step     3200 |     20 batches | lr 0.01 | ms/batch 122.88 | loss  0.98 | ppl     2.672
| epoch  13 step     3300 |    120 batches | lr 0.01 | ms/batch 95.17 | loss  4.88 | ppl   131.813
| epoch  13 step     3400 |    220 batches | lr 0.01 | ms/batch 95.16 | loss  4.81 | ppl   123.193
| train loss: 4.86 | train ppl 129.205
----------------------------------------------------------------------------------------------------
| Eval  13 at step     3445 | time: 27.99s | valid loss  4.80 | valid ppl   121.033
| Total time: 361.82s
----------------------------------------------------------------------------------------------------
| epoch  14 step     3500 |     55 batches | lr 0.01 | ms/batch 123.11 | loss  2.67 | ppl    14.501
| epoch  14 step     3600 |    155 batches | lr 0.01 | ms/batch 95.20 | loss  4.87 | ppl   130.931
| epoch  14 step     3700 |    255 batches | lr 0.01 | ms/batch 95.20 | loss  4.82 | ppl   124.181
| train loss: 4.85 | train ppl 128.236
----------------------------------------------------------------------------------------------------
| Eval  14 at step     3710 | time: 28.03s | valid loss  4.79 | valid ppl   120.064
| Total time: 389.76s
----------------------------------------------------------------------------------------------------
| epoch  15 step     3800 |     90 batches | lr 0.01 | ms/batch 122.83 | loss  4.38 | ppl    79.518
| epoch  15 step     3900 |    190 batches | lr 0.01 | ms/batch 95.17 | loss  4.82 | ppl   123.477
| train loss: 4.84 | train ppl 126.608
----------------------------------------------------------------------------------------------------
| Eval  15 at step     3975 | time: 27.99s | valid loss  4.79 | valid ppl   120.061
| Total time: 417.68s
----------------------------------------------------------------------------------------------------
| epoch  16 step     4000 |     25 batches | lr 0.01 | ms/batch 123.25 | loss  1.21 | ppl     3.369
| epoch  16 step     4100 |    125 batches | lr 0.01 | ms/batch 95.19 | loss  4.85 | ppl   127.201
| epoch  16 step     4200 |    225 batches | lr 0.01 | ms/batch 95.15 | loss  4.78 | ppl   119.228
| train loss: 4.82 | train ppl 124.583
----------------------------------------------------------------------------------------------------
| Eval  16 at step     4240 | time: 28.01s | valid loss  4.78 | valid ppl   118.689
| Total time: 445.61s
----------------------------------------------------------------------------------------------------
| epoch  17 step     4300 |     60 batches | lr 0.01 | ms/batch 123.03 | loss  2.90 | ppl    18.116
| epoch  17 step     4400 |    160 batches | lr 0.01 | ms/batch 95.20 | loss  4.84 | ppl   126.480
| epoch  17 step     4500 |    260 batches | lr 0.01 | ms/batch 95.19 | loss  4.80 | ppl   121.058
| train loss: 4.82 | train ppl 124.150
----------------------------------------------------------------------------------------------------
| Eval  17 at step     4505 | time: 28.01s | valid loss  4.77 | valid ppl   118.180
| Total time: 473.54s
----------------------------------------------------------------------------------------------------
| epoch  18 step     4600 |     95 batches | lr 0.01 | ms/batch 122.87 | loss  4.60 | ppl    99.038
| epoch  18 step     4700 |    195 batches | lr 0.01 | ms/batch 95.16 | loss  4.78 | ppl   119.541
| train loss: 4.81 | train ppl 123.257
----------------------------------------------------------------------------------------------------
| Eval  18 at step     4770 | time: 27.99s | valid loss  4.77 | valid ppl   117.715
| Total time: 501.44s
----------------------------------------------------------------------------------------------------
| epoch  19 step     4800 |     30 batches | lr 0.01 | ms/batch 122.92 | loss  1.45 | ppl     4.264
| epoch  19 step     4900 |    130 batches | lr 0.01 | ms/batch 95.20 | loss  4.83 | ppl   124.974
| epoch  19 step     5000 |    230 batches | lr 0.01 | ms/batch 95.15 | loss  4.77 | ppl   118.404
| train loss: 4.81 | train ppl 122.442
!! Converted Model !!
----------------------------------------------------------------------------------------------------
| Eval  19 at step     5035 | time: 203.73s | valid loss  4.68 | valid ppl   107.640
| Total time: 705.13s
----------------------------------------------------------------------------------------------------
| epoch  20 step     5100 |     65 batches | lr 0.01 | ms/batch 3787.58 | loss  3.09 | ppl    21.958
| epoch  20 step     5200 |    165 batches | lr 0.01 | ms/batch 3025.38 | loss  4.98 | ppl   145.768
| epoch  20 step     5300 |    265 batches | lr 0.01 | ms/batch 2990.09 | loss  4.92 | ppl   136.344
| train loss: 4.90 | train ppl 134.808
----------------------------------------------------------------------------------------------------
| Eval  20 at step     5300 | time: 876.90s | valid loss  4.76 | valid ppl   116.473
| Total time: 1582.24s
----------------------------------------------------------------------------------------------------
| epoch  21 step     5400 |    100 batches | lr 0.01 | ms/batch 3788.08 | loss  4.94 | ppl   139.574
| epoch  21 step     5500 |    200 batches | lr 0.01 | ms/batch 3019.69 | loss  4.83 | ppl   125.641
| train loss: 4.88 | train ppl 131.324
----------------------------------------------------------------------------------------------------
| Eval  21 at step     5565 | time: 874.27s | valid loss  4.69 | valid ppl   108.944
| Total time: 2456.82s
----------------------------------------------------------------------------------------------------
| epoch  22 step     5600 |     35 batches | lr 0.01 | ms/batch 3761.00 | loss  1.70 | ppl     5.479
| epoch  22 step     5700 |    135 batches | lr 0.01 | ms/batch 3020.91 | loss  4.79 | ppl   120.790
| epoch  22 step     5800 |    235 batches | lr 0.01 | ms/batch 3021.81 | loss  4.71 | ppl   111.604
| train loss: 4.77 | train ppl 118.180
----------------------------------------------------------------------------------------------------
| Eval  22 at step     5830 | time: 874.73s | valid loss  4.73 | valid ppl   113.754
| Total time: 3331.85s
----------------------------------------------------------------------------------------------------
| epoch  23 step     5900 |     70 batches | lr 0.01 | ms/batch 3760.57 | loss  3.32 | ppl    27.562
| epoch  23 step     6000 |    170 batches | lr 0.01 | ms/batch 3018.78 | loss  4.76 | ppl   116.976
| train loss: 4.73 | train ppl 112.804
----------------------------------------------------------------------------------------------------
| Eval  23 at step     6095 | time: 874.05s | valid loss  4.62 | valid ppl   101.710
| Total time: 4206.20s
----------------------------------------------------------------------------------------------------
| epoch  24 step     6100 |      5 batches | lr 0.01 | ms/batch 3774.32 | loss  0.25 | ppl     1.280
| epoch  24 step     6200 |    105 batches | lr 0.01 | ms/batch 3019.86 | loss  4.72 | ppl   111.615
| epoch  24 step     6300 |    205 batches | lr 0.01 | ms/batch 3020.72 | loss  4.67 | ppl   107.143
| train loss: 4.70 | train ppl 110.041
----------------------------------------------------------------------------------------------------
| Eval  24 at step     6360 | time: 875.92s | valid loss  4.62 | valid ppl   101.646
| Total time: 5082.38s
----------------------------------------------------------------------------------------------------
| epoch  25 step     6400 |     40 batches | lr 0.01 | ms/batch 3776.25 | loss  1.89 | ppl     6.650
| epoch  25 step     6500 |    140 batches | lr 0.01 | ms/batch 3020.01 | loss  4.69 | ppl   108.646
| epoch  25 step     6600 |    240 batches | lr 0.01 | ms/batch 3019.52 | loss  4.64 | ppl   103.098
| train loss: 4.68 | train ppl 107.539
----------------------------------------------------------------------------------------------------
| Eval  25 at step     6625 | time: 875.80s | valid loss  4.62 | valid ppl   101.141
| Total time: 5958.40s
----------------------------------------------------------------------------------------------------
| epoch  26 step     6700 |     75 batches | lr 0.01 | ms/batch 3775.41 | loss  3.51 | ppl    33.556
| epoch  26 step     6800 |    175 batches | lr 0.01 | ms/batch 3020.11 | loss  4.67 | ppl   106.209
| train loss: 4.66 | train ppl 105.451
----------------------------------------------------------------------------------------------------
| Eval  26 at step     6890 | time: 875.97s | valid loss  4.60 | valid ppl    99.111
| Total time: 6834.59s
----------------------------------------------------------------------------------------------------
| epoch  27 step     6900 |     10 batches | lr 0.01 | ms/batch 3776.50 | loss  0.48 | ppl     1.615
| epoch  27 step     7000 |    110 batches | lr 0.01 | ms/batch 3019.68 | loss  4.65 | ppl   104.895
| epoch  27 step     7100 |    210 batches | lr 0.01 | ms/batch 3019.64 | loss  4.60 | ppl    99.404
| train loss: 4.64 | train ppl 103.534
----------------------------------------------------------------------------------------------------
| Eval  27 at step     7155 | time: 875.77s | valid loss  4.59 | valid ppl    98.324
| Total time: 7710.57s
----------------------------------------------------------------------------------------------------
| epoch  28 step     7200 |     45 batches | lr 0.01 | ms/batch 3775.50 | loss  2.10 | ppl     8.179
| epoch  28 step     7300 |    145 batches | lr 0.01 | ms/batch 3020.41 | loss  4.63 | ppl   102.797
| epoch  28 step     7400 |    245 batches | lr 0.01 | ms/batch 3019.99 | loss  4.59 | ppl    98.870
| train loss: 4.63 | train ppl 102.333
----------------------------------------------------------------------------------------------------
| Eval  28 at step     7420 | time: 875.95s | valid loss  4.59 | valid ppl    98.987
| Total time: 8586.73s
----------------------------------------------------------------------------------------------------
| epoch  29 step     7500 |     80 batches | lr 0.01 | ms/batch 3760.44 | loss  3.72 | ppl    41.079
| epoch  29 step     7600 |    180 batches | lr 0.01 | ms/batch 3019.21 | loss  4.62 | ppl   101.602
| train loss: 4.62 | train ppl 101.497
----------------------------------------------------------------------------------------------------
| Eval  29 at step     7685 | time: 874.20s | valid loss  4.82 | valid ppl   123.385
| Total time: 9461.26s
----------------------------------------------------------------------------------------------------
| epoch  30 step     7700 |     15 batches | lr 0.001 | ms/batch 3759.94 | loss  0.71 | ppl     2.029
| epoch  30 step     7800 |    115 batches | lr 0.001 | ms/batch 3020.13 | loss  4.60 | ppl    99.382
| epoch  30 step     7900 |    215 batches | lr 0.001 | ms/batch 3019.85 | loss  4.40 | ppl    81.858
| train loss: 4.49 | train ppl 89.472
----------------------------------------------------------------------------------------------------
| Eval  30 at step     7950 | time: 874.35s | valid loss  4.57 | valid ppl    96.305
| Total time: 10335.91s
----------------------------------------------------------------------------------------------------
| epoch  31 step     8000 |     50 batches | lr 0.001 | ms/batch 3775.09 | loss  2.28 | ppl     9.787
| epoch  31 step     8100 |    150 batches | lr 0.001 | ms/batch 3019.53 | loss  4.49 | ppl    88.916
| epoch  31 step     8200 |    250 batches | lr 0.001 | ms/batch 3019.39 | loss  4.35 | ppl    77.138
| train loss: 4.45 | train ppl 85.457
----------------------------------------------------------------------------------------------------
| Eval  31 at step     8215 | time: 875.68s | valid loss  4.54 | valid ppl    93.338
| Total time: 11211.85s
----------------------------------------------------------------------------------------------------
| epoch  32 step     8300 |     85 batches | lr 0.001 | ms/batch 3775.08 | loss  3.84 | ppl    46.382
| epoch  32 step     8400 |    185 batches | lr 0.001 | ms/batch 3019.77 | loss  4.42 | ppl    83.119
| train loss: 4.43 | train ppl 84.109
----------------------------------------------------------------------------------------------------
| Eval  32 at step     8480 | time: 875.74s | valid loss  4.62 | valid ppl   101.232
| Total time: 12087.79s
----------------------------------------------------------------------------------------------------
| epoch  33 step     8500 |     20 batches | lr 0.001 | ms/batch 3759.68 | loss  0.91 | ppl     2.482
| epoch  33 step     8600 |    120 batches | lr 0.001 | ms/batch 3018.99 | loss  4.46 | ppl    86.843
| epoch  33 step     8700 |    220 batches | lr 0.001 | ms/batch 3018.66 | loss  4.32 | ppl    75.392
| train loss: 4.40 | train ppl 81.576
----------------------------------------------------------------------------------------------------
| Eval  33 at step     8745 | time: 874.04s | valid loss  4.59 | valid ppl    98.029
| Total time: 12962.14s
----------------------------------------------------------------------------------------------------
| epoch  34 step     8800 |     55 batches | lr 0.001 | ms/batch 3760.14 | loss  2.44 | ppl    11.526
| epoch  34 step     8900 |    155 batches | lr 0.001 | ms/batch 3020.54 | loss  4.43 | ppl    83.803
| epoch  34 step     9000 |    255 batches | lr 0.001 | ms/batch 3020.59 | loss  4.31 | ppl    74.266
| train loss: 4.39 | train ppl 80.348
----------------------------------------------------------------------------------------------------
| Eval  34 at step     9010 | time: 874.55s | valid loss  4.51 | valid ppl    91.254
| Total time: 13836.99s
----------------------------------------------------------------------------------------------------
| epoch  35 step     9100 |     90 batches | lr 0.001 | ms/batch 3775.18 | loss  3.99 | ppl    54.087
| epoch  35 step     9200 |    190 batches | lr 0.001 | ms/batch 3019.85 | loss  4.34 | ppl    76.784
| train loss: 4.37 | train ppl 78.888
----------------------------------------------------------------------------------------------------
| Eval  35 at step     9275 | time: 875.80s | valid loss  4.55 | valid ppl    95.009
| Total time: 14713.08s
----------------------------------------------------------------------------------------------------
| epoch  36 step     9300 |     25 batches | lr 0.001 | ms/batch 3761.33 | loss  1.11 | ppl     3.039
| epoch  36 step     9400 |    125 batches | lr 0.001 | ms/batch 3020.14 | loss  4.41 | ppl    81.982
| epoch  36 step     9500 |    225 batches | lr 0.001 | ms/batch 3019.24 | loss  4.30 | ppl    73.484
| train loss: 4.36 | train ppl 78.209
----------------------------------------------------------------------------------------------------
| Eval  36 at step     9540 | time: 874.42s | valid loss  4.55 | valid ppl    94.890
| Total time: 15587.80s
----------------------------------------------------------------------------------------------------
| epoch  37 step     9600 |     60 batches | lr 0.001 | ms/batch 3761.41 | loss  2.64 | ppl    14.033
| epoch  37 step     9700 |    160 batches | lr 0.001 | ms/batch 3020.37 | loss  4.38 | ppl    79.491
| epoch  37 step     9800 |    260 batches | lr 0.001 | ms/batch 3020.52 | loss  4.28 | ppl    71.966
| train loss: 4.34 | train ppl 77.045
----------------------------------------------------------------------------------------------------
| Eval  37 at step     9805 | time: 874.52s | valid loss  4.53 | valid ppl    92.300
| Total time: 16462.62s
----------------------------------------------------------------------------------------------------
====================================================================================================
| End of training | test loss  4.48 | test ppl    87.945
| Maximum memory usage: 5569.20MB
====================================================================================================
