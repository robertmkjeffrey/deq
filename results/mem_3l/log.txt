|Basic Memory Usage: 8.69MB current, 8.69MB max
====================================================================================================
    - data : ../data/penn/
    - dataset : ptb
    - n_layer : 3
    - d_embed : 400
    - nhid : 1000
    - nout : 400
    - epochs : 500
    - halt_ppl : None
    - time_limit : 125000.0
    - optim : Adam
    - lr : 0.01
    - clip : 0.225
    - batch_size : 32
    - batch_chunk : 1
    - seq_len : 110
    - subseq_len : 55
    - dropout : 0.45
    - dropouti : 0.45
    - wdrop : 0.5
    - emb_dropout : 0.1
    - dropouth : 0.28
    - weight_decay : 1.2e-06
    - wnorm : True
    - seed : 9999
    - cuda : True
    - not_tied : False
    - anneal : 10
    - log_interval : 100
    - force_deq_validation : False
    - when : [-1]
    - ksize : 2
    - dilation : 1
    - n_experts : 0
    - multi_gpu : False
    - use_gpus : None
    - f_thres : 45
    - b_thres : 45
    - work_dir : LM-TREdeq-ptb/mem_3l_20210529-223346
    - restart : False
    - restart_dir : 
    - debug : False
    - gpu0_bsz : 7
    - pretrain_steps : 15000
    - start_train_steps : 0
    - timing : True
    - eval_mem : True
    - eval : False
    - load : 
    - name : mem_3l
    - tied : True
    - n_token : 10000
    - n_all_param : 24193603
====================================================================================================
#params = 24193603
|Static Memory Usage: 252.25MB current, 269.03MB max
| epoch  -1 step      100 |    100 batches | lr 0.01 | ms/batch 97.57 | loss  6.34 | ppl   569.063
| epoch  -1 step      200 |    200 batches | lr 0.01 | ms/batch 95.19 | loss  5.75 | ppl   313.759
| train loss: 5.95 | train ppl 383.696
|Training Memory Usage: 1413.24MB
|Training Inference Memory Usage: 1054.22MB
|Validation Memory Usage: 704.05MB
|Test Memory Usage: 704.05MB
